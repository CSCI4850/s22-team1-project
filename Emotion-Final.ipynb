{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KQ6bF4srq4jA"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential #Initialise our neural network model as a sequential network\n",
    "from tensorflow.keras.layers import Conv2D #Convolution operation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Activation#Applies activation function\n",
    "from tensorflow.keras.layers import Dropout#Prevents overfitting by randomly converting few outputs to zero\n",
    "from tensorflow.keras.layers import MaxPooling2D # Maxpooling function\n",
    "from tensorflow.keras.layers import Flatten # Converting 2D arrays into a 1D linear vector\n",
    "from tensorflow.keras.layers import Dense # Regular fully connected neural network\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L1pnmQpH00b_"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "  \n",
    "  #classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprsie', 'Neutral']  #We will be dealing with seven different types of emotions.\n",
    "\n",
    "  data = []\n",
    "  test_data = []\n",
    "  test_labels = []\n",
    "  labels =[]\n",
    "\n",
    "  with open(dataset_path, 'r') as file:\n",
    "      for line_no, line in enumerate(file.readlines()):\n",
    "          if 0 < line_no <= 35887:\n",
    "            curr_class, line, set_type = line.split(',')\n",
    "            image_data = np.asarray([int(x) for x in line.split()]).reshape(48, 48)#Creating a list out of the string then converting it into a 2-Dimensional numpy array.\n",
    "            image_data =image_data.astype(np.uint8)/255.0\n",
    "            \n",
    "            if (set_type.strip() == 'PrivateTest'):\n",
    "              \n",
    "              test_data.append(image_data)\n",
    "              test_labels.append(curr_class)\n",
    "            else:\n",
    "              data.append(image_data)\n",
    "              labels.append(curr_class)\n",
    "      \n",
    "      test_data = np.expand_dims(test_data, -1)\n",
    "      test_labels = to_categorical(test_labels, num_classes = 7)\n",
    "      data = np.expand_dims(data, -1)   \n",
    "      labels = to_categorical(labels, num_classes = 7)\n",
    "    \n",
    "      return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 161] The specified path is invalid: '/\\\\~MTSU'\n",
      "C:\\Users\\brice\\My Drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktYf3wTtEBIu",
    "outputId": "f6b0465a-1e22-4bd1-debb-d8107afe4549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in Training set: 32298\n",
      "Number of images in Test set: 3589\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"~MTSU/csci4850-NeuralNets/proj/archive (2)/fer2013.csv\" \n",
    "train_data, train_labels, test_data, test_labels = load_data(dataset_path)\n",
    "#train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size,random_state = seed)\n",
    "\n",
    "print(\"Number of images in Training set:\", len(train_data))\n",
    "print(\"Number of images in Test set:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iW2cYr8VrW1o",
    "outputId": "39a42c82-f445-4e64-bb33-d8eb711b49cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 64)        640       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 46, 46, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 23, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 23, 23, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 23, 23, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 23, 23, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 23, 23, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 23, 23, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 23, 23, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 11, 11, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 11, 11, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 11, 11, 128)       36992     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 11, 11, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 11, 11, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 11, 11, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 11, 11, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1638912   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,276,327\n",
      "Trainable params: 2,275,111\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#######HYPERPARAMATERS###########\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "#################################\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "#model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "#odel.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "    \n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "adam = optimizers.Adam(learning_rate = learning_rate)\n",
    "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "print(model.summary())\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto')\n",
    "checkpointer = ModelCheckpoint('/content/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSEpyQ-Ezk6S",
    "outputId": "b6c9dc28-33c7-4cf9-dbe0-2ebf41d410fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.9506 - accuracy: 0.2076\n",
      "Epoch 1: val_loss improved from inf to 1.84404, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 186s 460ms/step - loss: 1.9506 - accuracy: 0.2076 - val_loss: 1.8440 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.8564 - accuracy: 0.2374\n",
      "Epoch 2: val_loss improved from 1.84404 to 1.84207, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 185s 457ms/step - loss: 1.8564 - accuracy: 0.2374 - val_loss: 1.8421 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.8363 - accuracy: 0.2479\n",
      "Epoch 3: val_loss improved from 1.84207 to 1.81899, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 179s 444ms/step - loss: 1.8363 - accuracy: 0.2479 - val_loss: 1.8190 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.8264 - accuracy: 0.2491\n",
      "Epoch 4: val_loss improved from 1.81899 to 1.81389, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 187s 464ms/step - loss: 1.8264 - accuracy: 0.2491 - val_loss: 1.8139 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.8167 - accuracy: 0.2508\n",
      "Epoch 5: val_loss improved from 1.81389 to 1.79947, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 193s 478ms/step - loss: 1.8167 - accuracy: 0.2508 - val_loss: 1.7995 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.8084 - accuracy: 0.2513\n",
      "Epoch 6: val_loss improved from 1.79947 to 1.79182, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 196s 486ms/step - loss: 1.8084 - accuracy: 0.2513 - val_loss: 1.7918 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.7795 - accuracy: 0.2548\n",
      "Epoch 7: val_loss did not improve from 1.79182\n",
      "404/404 [==============================] - 188s 465ms/step - loss: 1.7795 - accuracy: 0.2548 - val_loss: 1.8047 - val_accuracy: 0.2910 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.7473 - accuracy: 0.2894\n",
      "Epoch 8: val_loss improved from 1.79182 to 1.78297, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 186s 460ms/step - loss: 1.7473 - accuracy: 0.2894 - val_loss: 1.7830 - val_accuracy: 0.2842 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.7071 - accuracy: 0.3157\n",
      "Epoch 9: val_loss improved from 1.78297 to 1.71776, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 191s 473ms/step - loss: 1.7071 - accuracy: 0.3157 - val_loss: 1.7178 - val_accuracy: 0.3056 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.6678 - accuracy: 0.3384\n",
      "Epoch 10: val_loss improved from 1.71776 to 1.60068, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 217s 536ms/step - loss: 1.6678 - accuracy: 0.3384 - val_loss: 1.6007 - val_accuracy: 0.3738 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.6269 - accuracy: 0.3571\n",
      "Epoch 11: val_loss did not improve from 1.60068\n",
      "404/404 [==============================] - 168s 416ms/step - loss: 1.6269 - accuracy: 0.3571 - val_loss: 1.7275 - val_accuracy: 0.2994 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.5976 - accuracy: 0.3694\n",
      "Epoch 12: val_loss improved from 1.60068 to 1.59520, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 171s 423ms/step - loss: 1.5976 - accuracy: 0.3694 - val_loss: 1.5952 - val_accuracy: 0.3861 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.5805 - accuracy: 0.3834\n",
      "Epoch 13: val_loss did not improve from 1.59520\n",
      "404/404 [==============================] - 169s 418ms/step - loss: 1.5805 - accuracy: 0.3834 - val_loss: 1.7539 - val_accuracy: 0.2890 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.5429 - accuracy: 0.3901\n",
      "Epoch 14: val_loss improved from 1.59520 to 1.53038, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 179s 444ms/step - loss: 1.5429 - accuracy: 0.3901 - val_loss: 1.5304 - val_accuracy: 0.4181 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.5340 - accuracy: 0.4003\n",
      "Epoch 15: val_loss improved from 1.53038 to 1.51753, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 187s 464ms/step - loss: 1.5340 - accuracy: 0.4003 - val_loss: 1.5175 - val_accuracy: 0.4457 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.5199 - accuracy: 0.4009\n",
      "Epoch 16: val_loss did not improve from 1.51753\n",
      "404/404 [==============================] - 190s 470ms/step - loss: 1.5199 - accuracy: 0.4009 - val_loss: 1.5707 - val_accuracy: 0.4341 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.5001 - accuracy: 0.4080\n",
      "Epoch 17: val_loss improved from 1.51753 to 1.50076, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 192s 476ms/step - loss: 1.5001 - accuracy: 0.4080 - val_loss: 1.5008 - val_accuracy: 0.4387 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4863 - accuracy: 0.4147\n",
      "Epoch 18: val_loss improved from 1.50076 to 1.47815, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 193s 477ms/step - loss: 1.4863 - accuracy: 0.4147 - val_loss: 1.4782 - val_accuracy: 0.4364 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4752 - accuracy: 0.4155\n",
      "Epoch 19: val_loss did not improve from 1.47815\n",
      "404/404 [==============================] - 185s 459ms/step - loss: 1.4752 - accuracy: 0.4155 - val_loss: 1.5146 - val_accuracy: 0.4102 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4681 - accuracy: 0.4179\n",
      "Epoch 20: val_loss improved from 1.47815 to 1.43252, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 185s 458ms/step - loss: 1.4681 - accuracy: 0.4179 - val_loss: 1.4325 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4477 - accuracy: 0.4277\n",
      "Epoch 21: val_loss did not improve from 1.43252\n",
      "404/404 [==============================] - 180s 445ms/step - loss: 1.4477 - accuracy: 0.4277 - val_loss: 1.6750 - val_accuracy: 0.3754 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4363 - accuracy: 0.4338\n",
      "Epoch 22: val_loss did not improve from 1.43252\n",
      "404/404 [==============================] - 189s 467ms/step - loss: 1.4363 - accuracy: 0.4338 - val_loss: 1.5371 - val_accuracy: 0.4091 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4347 - accuracy: 0.4341\n",
      "Epoch 23: val_loss did not improve from 1.43252\n",
      "404/404 [==============================] - 181s 448ms/step - loss: 1.4347 - accuracy: 0.4341 - val_loss: 1.4573 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4296 - accuracy: 0.4380\n",
      "Epoch 24: val_loss improved from 1.43252 to 1.40008, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 181s 448ms/step - loss: 1.4296 - accuracy: 0.4380 - val_loss: 1.4001 - val_accuracy: 0.4740 - lr: 9.0000e-04\n",
      "Epoch 25/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4081 - accuracy: 0.4462\n",
      "Epoch 25: val_loss did not improve from 1.40008\n",
      "404/404 [==============================] - 167s 412ms/step - loss: 1.4081 - accuracy: 0.4462 - val_loss: 1.4029 - val_accuracy: 0.4514 - lr: 9.0000e-04\n",
      "Epoch 26/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.4112 - accuracy: 0.4427\n",
      "Epoch 26: val_loss improved from 1.40008 to 1.37428, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 172s 426ms/step - loss: 1.4112 - accuracy: 0.4427 - val_loss: 1.3743 - val_accuracy: 0.4706 - lr: 9.0000e-04\n",
      "Epoch 27/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3958 - accuracy: 0.4524\n",
      "Epoch 27: val_loss improved from 1.37428 to 1.37046, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 189s 467ms/step - loss: 1.3958 - accuracy: 0.4524 - val_loss: 1.3705 - val_accuracy: 0.4673 - lr: 9.0000e-04\n",
      "Epoch 28/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3870 - accuracy: 0.4539\n",
      "Epoch 28: val_loss did not improve from 1.37046\n",
      "404/404 [==============================] - 169s 419ms/step - loss: 1.3870 - accuracy: 0.4539 - val_loss: 1.3908 - val_accuracy: 0.4762 - lr: 9.0000e-04\n",
      "Epoch 29/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3845 - accuracy: 0.4574\n",
      "Epoch 29: val_loss did not improve from 1.37046\n",
      "404/404 [==============================] - 169s 419ms/step - loss: 1.3845 - accuracy: 0.4574 - val_loss: 1.3965 - val_accuracy: 0.4802 - lr: 9.0000e-04\n",
      "Epoch 30/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3776 - accuracy: 0.4578\n",
      "Epoch 30: val_loss improved from 1.37046 to 1.32294, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 171s 424ms/step - loss: 1.3776 - accuracy: 0.4578 - val_loss: 1.3229 - val_accuracy: 0.4935 - lr: 9.0000e-04\n",
      "Epoch 31/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3754 - accuracy: 0.4624\n",
      "Epoch 31: val_loss did not improve from 1.32294\n",
      "404/404 [==============================] - 172s 426ms/step - loss: 1.3754 - accuracy: 0.4624 - val_loss: 1.3933 - val_accuracy: 0.4652 - lr: 9.0000e-04\n",
      "Epoch 32/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3694 - accuracy: 0.4635\n",
      "Epoch 32: val_loss did not improve from 1.32294\n",
      "404/404 [==============================] - 171s 424ms/step - loss: 1.3694 - accuracy: 0.4635 - val_loss: 1.5775 - val_accuracy: 0.3940 - lr: 9.0000e-04\n",
      "Epoch 33/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3641 - accuracy: 0.4706\n",
      "Epoch 33: val_loss did not improve from 1.32294\n",
      "404/404 [==============================] - 171s 423ms/step - loss: 1.3641 - accuracy: 0.4706 - val_loss: 1.3409 - val_accuracy: 0.4921 - lr: 9.0000e-04\n",
      "Epoch 34/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3475 - accuracy: 0.4775\n",
      "Epoch 34: val_loss did not improve from 1.32294\n",
      "404/404 [==============================] - 172s 426ms/step - loss: 1.3475 - accuracy: 0.4775 - val_loss: 1.3983 - val_accuracy: 0.4986 - lr: 8.1000e-04\n",
      "Epoch 35/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3413 - accuracy: 0.4800\n",
      "Epoch 35: val_loss improved from 1.32294 to 1.29802, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 176s 434ms/step - loss: 1.3413 - accuracy: 0.4800 - val_loss: 1.2980 - val_accuracy: 0.4949 - lr: 8.1000e-04\n",
      "Epoch 36/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3283 - accuracy: 0.4883\n",
      "Epoch 36: val_loss did not improve from 1.29802\n",
      "404/404 [==============================] - 178s 441ms/step - loss: 1.3283 - accuracy: 0.4883 - val_loss: 1.3064 - val_accuracy: 0.5025 - lr: 8.1000e-04\n",
      "Epoch 37/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3293 - accuracy: 0.4844\n",
      "Epoch 37: val_loss did not improve from 1.29802\n",
      "404/404 [==============================] - 176s 437ms/step - loss: 1.3293 - accuracy: 0.4844 - val_loss: 1.3435 - val_accuracy: 0.4998 - lr: 8.1000e-04\n",
      "Epoch 38/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3169 - accuracy: 0.4959\n",
      "Epoch 38: val_loss did not improve from 1.29802\n",
      "404/404 [==============================] - 176s 436ms/step - loss: 1.3169 - accuracy: 0.4959 - val_loss: 1.3255 - val_accuracy: 0.4943 - lr: 8.1000e-04\n",
      "Epoch 39/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3083 - accuracy: 0.4996\n",
      "Epoch 39: val_loss improved from 1.29802 to 1.28603, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 179s 444ms/step - loss: 1.3083 - accuracy: 0.4996 - val_loss: 1.2860 - val_accuracy: 0.5020 - lr: 7.2900e-04\n",
      "Epoch 40/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.3030 - accuracy: 0.5029\n",
      "Epoch 40: val_loss did not improve from 1.28603\n",
      "404/404 [==============================] - 181s 448ms/step - loss: 1.3030 - accuracy: 0.5029 - val_loss: 1.3509 - val_accuracy: 0.5053 - lr: 7.2900e-04\n",
      "Epoch 41/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2905 - accuracy: 0.5066\n",
      "Epoch 41: val_loss improved from 1.28603 to 1.27416, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 183s 452ms/step - loss: 1.2905 - accuracy: 0.5066 - val_loss: 1.2742 - val_accuracy: 0.5283 - lr: 7.2900e-04\n",
      "Epoch 42/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2893 - accuracy: 0.5083\n",
      "Epoch 42: val_loss did not improve from 1.27416\n",
      "404/404 [==============================] - 179s 443ms/step - loss: 1.2893 - accuracy: 0.5083 - val_loss: 1.3071 - val_accuracy: 0.5353 - lr: 7.2900e-04\n",
      "Epoch 43/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2925 - accuracy: 0.5104\n",
      "Epoch 43: val_loss improved from 1.27416 to 1.26626, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 176s 435ms/step - loss: 1.2925 - accuracy: 0.5104 - val_loss: 1.2663 - val_accuracy: 0.5372 - lr: 7.2900e-04\n",
      "Epoch 44/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2782 - accuracy: 0.5148\n",
      "Epoch 44: val_loss improved from 1.26626 to 1.25018, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 189s 468ms/step - loss: 1.2782 - accuracy: 0.5148 - val_loss: 1.2502 - val_accuracy: 0.5302 - lr: 7.2900e-04\n",
      "Epoch 45/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2617 - accuracy: 0.5243\n",
      "Epoch 45: val_loss improved from 1.25018 to 1.24317, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 185s 459ms/step - loss: 1.2617 - accuracy: 0.5243 - val_loss: 1.2432 - val_accuracy: 0.5415 - lr: 7.2900e-04\n",
      "Epoch 46/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2727 - accuracy: 0.5231\n",
      "Epoch 46: val_loss improved from 1.24317 to 1.23854, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 207s 513ms/step - loss: 1.2727 - accuracy: 0.5231 - val_loss: 1.2385 - val_accuracy: 0.5485 - lr: 7.2900e-04\n",
      "Epoch 47/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2636 - accuracy: 0.5257\n",
      "Epoch 47: val_loss did not improve from 1.23854\n",
      "404/404 [==============================] - 232s 576ms/step - loss: 1.2636 - accuracy: 0.5257 - val_loss: 1.2697 - val_accuracy: 0.5347 - lr: 7.2900e-04\n",
      "Epoch 48/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2455 - accuracy: 0.5307\n",
      "Epoch 48: val_loss improved from 1.23854 to 1.20441, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 238s 588ms/step - loss: 1.2455 - accuracy: 0.5307 - val_loss: 1.2044 - val_accuracy: 0.5545 - lr: 7.2900e-04\n",
      "Epoch 49/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2414 - accuracy: 0.5328\n",
      "Epoch 49: val_loss did not improve from 1.20441\n",
      "404/404 [==============================] - 186s 461ms/step - loss: 1.2414 - accuracy: 0.5328 - val_loss: 1.2289 - val_accuracy: 0.5464 - lr: 7.2900e-04\n",
      "Epoch 50/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2358 - accuracy: 0.5364\n",
      "Epoch 50: val_loss improved from 1.20441 to 1.20110, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 175s 434ms/step - loss: 1.2358 - accuracy: 0.5364 - val_loss: 1.2011 - val_accuracy: 0.5653 - lr: 7.2900e-04\n",
      "Epoch 51/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2236 - accuracy: 0.5392\n",
      "Epoch 51: val_loss did not improve from 1.20110\n",
      "404/404 [==============================] - 250s 618ms/step - loss: 1.2236 - accuracy: 0.5392 - val_loss: 1.2734 - val_accuracy: 0.5241 - lr: 7.2900e-04\n",
      "Epoch 52/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2187 - accuracy: 0.5436\n",
      "Epoch 52: val_loss did not improve from 1.20110\n",
      "404/404 [==============================] - 252s 623ms/step - loss: 1.2187 - accuracy: 0.5436 - val_loss: 1.2281 - val_accuracy: 0.5577 - lr: 7.2900e-04\n",
      "Epoch 53/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2171 - accuracy: 0.5477\n",
      "Epoch 53: val_loss improved from 1.20110 to 1.17719, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 255s 632ms/step - loss: 1.2171 - accuracy: 0.5477 - val_loss: 1.1772 - val_accuracy: 0.5675 - lr: 7.2900e-04\n",
      "Epoch 54/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2172 - accuracy: 0.5476\n",
      "Epoch 54: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 248s 614ms/step - loss: 1.2172 - accuracy: 0.5476 - val_loss: 1.2231 - val_accuracy: 0.5659 - lr: 7.2900e-04\n",
      "Epoch 55/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2153 - accuracy: 0.5478\n",
      "Epoch 55: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 240s 594ms/step - loss: 1.2153 - accuracy: 0.5478 - val_loss: 1.1917 - val_accuracy: 0.5650 - lr: 7.2900e-04\n",
      "Epoch 56/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.2046 - accuracy: 0.5548\n",
      "Epoch 56: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 233s 578ms/step - loss: 1.2046 - accuracy: 0.5548 - val_loss: 1.1869 - val_accuracy: 0.5718 - lr: 7.2900e-04\n",
      "Epoch 57/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1877 - accuracy: 0.5604\n",
      "Epoch 57: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 234s 580ms/step - loss: 1.1877 - accuracy: 0.5604 - val_loss: 1.1860 - val_accuracy: 0.5738 - lr: 6.5610e-04\n",
      "Epoch 58/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1723 - accuracy: 0.5672\n",
      "Epoch 58: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 233s 577ms/step - loss: 1.1723 - accuracy: 0.5672 - val_loss: 1.2053 - val_accuracy: 0.5748 - lr: 6.5610e-04\n",
      "Epoch 59/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1715 - accuracy: 0.5677\n",
      "Epoch 59: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 233s 576ms/step - loss: 1.1715 - accuracy: 0.5677 - val_loss: 1.1963 - val_accuracy: 0.5627 - lr: 6.5610e-04\n",
      "Epoch 60/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1699 - accuracy: 0.5702\n",
      "Epoch 60: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 233s 576ms/step - loss: 1.1699 - accuracy: 0.5702 - val_loss: 1.2627 - val_accuracy: 0.5485 - lr: 5.9049e-04\n",
      "Epoch 61/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1576 - accuracy: 0.5724\n",
      "Epoch 61: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 229s 568ms/step - loss: 1.1576 - accuracy: 0.5724 - val_loss: 1.1782 - val_accuracy: 0.5830 - lr: 5.9049e-04\n",
      "Epoch 62/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1510 - accuracy: 0.5744\n",
      "Epoch 62: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 233s 576ms/step - loss: 1.1510 - accuracy: 0.5744 - val_loss: 1.1848 - val_accuracy: 0.5741 - lr: 5.9049e-04\n",
      "Epoch 63/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1390 - accuracy: 0.5760\n",
      "Epoch 63: val_loss did not improve from 1.17719\n",
      "404/404 [==============================] - 229s 567ms/step - loss: 1.1390 - accuracy: 0.5760 - val_loss: 1.2108 - val_accuracy: 0.5675 - lr: 5.3144e-04\n",
      "Epoch 64/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1309 - accuracy: 0.5801\n",
      "Epoch 64: val_loss improved from 1.17719 to 1.16476, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 233s 577ms/step - loss: 1.1309 - accuracy: 0.5801 - val_loss: 1.1648 - val_accuracy: 0.5762 - lr: 5.3144e-04\n",
      "Epoch 65/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1315 - accuracy: 0.5832\n",
      "Epoch 65: val_loss did not improve from 1.16476\n",
      "404/404 [==============================] - 246s 609ms/step - loss: 1.1315 - accuracy: 0.5832 - val_loss: 1.1856 - val_accuracy: 0.5820 - lr: 5.3144e-04\n",
      "Epoch 66/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1240 - accuracy: 0.5829\n",
      "Epoch 66: val_loss improved from 1.16476 to 1.14761, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 262s 650ms/step - loss: 1.1240 - accuracy: 0.5829 - val_loss: 1.1476 - val_accuracy: 0.5819 - lr: 5.3144e-04\n",
      "Epoch 67/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1138 - accuracy: 0.5925\n",
      "Epoch 67: val_loss did not improve from 1.14761\n",
      "404/404 [==============================] - 275s 682ms/step - loss: 1.1138 - accuracy: 0.5925 - val_loss: 1.1722 - val_accuracy: 0.5791 - lr: 5.3144e-04\n",
      "Epoch 68/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1057 - accuracy: 0.5883\n",
      "Epoch 68: val_loss did not improve from 1.14761\n",
      "404/404 [==============================] - 279s 691ms/step - loss: 1.1057 - accuracy: 0.5883 - val_loss: 1.1866 - val_accuracy: 0.5734 - lr: 5.3144e-04\n",
      "Epoch 69/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1081 - accuracy: 0.5909\n",
      "Epoch 69: val_loss did not improve from 1.14761\n",
      "404/404 [==============================] - 281s 696ms/step - loss: 1.1081 - accuracy: 0.5909 - val_loss: 1.1827 - val_accuracy: 0.5728 - lr: 5.3144e-04\n",
      "Epoch 70/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.1069 - accuracy: 0.5920\n",
      "Epoch 70: val_loss did not improve from 1.14761\n",
      "404/404 [==============================] - 275s 682ms/step - loss: 1.1069 - accuracy: 0.5920 - val_loss: 1.1868 - val_accuracy: 0.5782 - lr: 4.7830e-04\n",
      "Epoch 71/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0935 - accuracy: 0.5981\n",
      "Epoch 71: val_loss did not improve from 1.14761\n",
      "404/404 [==============================] - 261s 646ms/step - loss: 1.0935 - accuracy: 0.5981 - val_loss: 1.1713 - val_accuracy: 0.5740 - lr: 4.7830e-04\n",
      "Epoch 72/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0954 - accuracy: 0.5963\n",
      "Epoch 72: val_loss did not improve from 1.14761\n",
      "404/404 [==============================] - 259s 641ms/step - loss: 1.0954 - accuracy: 0.5963 - val_loss: 1.2582 - val_accuracy: 0.5497 - lr: 4.7830e-04\n",
      "Epoch 73/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0822 - accuracy: 0.6019\n",
      "Epoch 73: val_loss improved from 1.14761 to 1.14049, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 261s 647ms/step - loss: 1.0822 - accuracy: 0.6019 - val_loss: 1.1405 - val_accuracy: 0.5862 - lr: 4.3047e-04\n",
      "Epoch 74/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0705 - accuracy: 0.6043\n",
      "Epoch 74: val_loss did not improve from 1.14049\n",
      "404/404 [==============================] - 256s 633ms/step - loss: 1.0705 - accuracy: 0.6043 - val_loss: 1.1533 - val_accuracy: 0.5943 - lr: 4.3047e-04\n",
      "Epoch 75/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0706 - accuracy: 0.6073\n",
      "Epoch 75: val_loss did not improve from 1.14049\n",
      "404/404 [==============================] - 250s 620ms/step - loss: 1.0706 - accuracy: 0.6073 - val_loss: 1.1482 - val_accuracy: 0.5941 - lr: 4.3047e-04\n",
      "Epoch 76/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0623 - accuracy: 0.6103\n",
      "Epoch 76: val_loss did not improve from 1.14049\n",
      "404/404 [==============================] - 251s 621ms/step - loss: 1.0623 - accuracy: 0.6103 - val_loss: 1.1544 - val_accuracy: 0.5889 - lr: 4.3047e-04\n",
      "Epoch 77/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.6136\n",
      "Epoch 77: val_loss did not improve from 1.14049\n",
      "404/404 [==============================] - 248s 613ms/step - loss: 1.0530 - accuracy: 0.6136 - val_loss: 1.1497 - val_accuracy: 0.5920 - lr: 3.8742e-04\n",
      "Epoch 78/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0508 - accuracy: 0.6128\n",
      "Epoch 78: val_loss improved from 1.14049 to 1.13675, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 244s 603ms/step - loss: 1.0508 - accuracy: 0.6128 - val_loss: 1.1367 - val_accuracy: 0.5961 - lr: 3.8742e-04\n",
      "Epoch 79/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0431 - accuracy: 0.6171\n",
      "Epoch 79: val_loss improved from 1.13675 to 1.13369, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 255s 632ms/step - loss: 1.0431 - accuracy: 0.6171 - val_loss: 1.1337 - val_accuracy: 0.6031 - lr: 3.8742e-04\n",
      "Epoch 80/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0398 - accuracy: 0.6208\n",
      "Epoch 80: val_loss improved from 1.13369 to 1.12343, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 258s 638ms/step - loss: 1.0398 - accuracy: 0.6208 - val_loss: 1.1234 - val_accuracy: 0.5994 - lr: 3.8742e-04\n",
      "Epoch 81/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0357 - accuracy: 0.6229\n",
      "Epoch 81: val_loss improved from 1.12343 to 1.11853, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 243s 602ms/step - loss: 1.0357 - accuracy: 0.6229 - val_loss: 1.1185 - val_accuracy: 0.6051 - lr: 3.8742e-04\n",
      "Epoch 82/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0257 - accuracy: 0.6226\n",
      "Epoch 82: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 231s 571ms/step - loss: 1.0257 - accuracy: 0.6226 - val_loss: 1.1245 - val_accuracy: 0.6063 - lr: 3.8742e-04\n",
      "Epoch 83/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0331 - accuracy: 0.6214\n",
      "Epoch 83: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 230s 570ms/step - loss: 1.0331 - accuracy: 0.6214 - val_loss: 1.1435 - val_accuracy: 0.5983 - lr: 3.8742e-04\n",
      "Epoch 84/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0300 - accuracy: 0.6229\n",
      "Epoch 84: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 231s 571ms/step - loss: 1.0300 - accuracy: 0.6229 - val_loss: 1.1445 - val_accuracy: 0.5946 - lr: 3.8742e-04\n",
      "Epoch 85/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0174 - accuracy: 0.6291\n",
      "Epoch 85: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 231s 571ms/step - loss: 1.0174 - accuracy: 0.6291 - val_loss: 1.1582 - val_accuracy: 0.5943 - lr: 3.4868e-04\n",
      "Epoch 86/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.6285\n",
      "Epoch 86: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 232s 574ms/step - loss: 1.0068 - accuracy: 0.6285 - val_loss: 1.1258 - val_accuracy: 0.6051 - lr: 3.4868e-04\n",
      "Epoch 87/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0130 - accuracy: 0.6303\n",
      "Epoch 87: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 231s 571ms/step - loss: 1.0130 - accuracy: 0.6303 - val_loss: 1.1193 - val_accuracy: 0.6080 - lr: 3.4868e-04\n",
      "Epoch 88/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 1.0056 - accuracy: 0.6316\n",
      "Epoch 88: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 230s 571ms/step - loss: 1.0056 - accuracy: 0.6316 - val_loss: 1.1311 - val_accuracy: 0.6073 - lr: 3.1381e-04\n",
      "Epoch 89/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9875 - accuracy: 0.6377\n",
      "Epoch 89: val_loss did not improve from 1.11853\n",
      "404/404 [==============================] - 231s 571ms/step - loss: 0.9875 - accuracy: 0.6377 - val_loss: 1.1275 - val_accuracy: 0.6099 - lr: 3.1381e-04\n",
      "Epoch 90/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9896 - accuracy: 0.6367\n",
      "Epoch 90: val_loss improved from 1.11853 to 1.11792, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 233s 577ms/step - loss: 0.9896 - accuracy: 0.6367 - val_loss: 1.1179 - val_accuracy: 0.6048 - lr: 3.1381e-04\n",
      "Epoch 91/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9881 - accuracy: 0.6406\n",
      "Epoch 91: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 235s 581ms/step - loss: 0.9881 - accuracy: 0.6406 - val_loss: 1.1323 - val_accuracy: 0.6065 - lr: 3.1381e-04\n",
      "Epoch 92/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9858 - accuracy: 0.6402\n",
      "Epoch 92: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 237s 586ms/step - loss: 0.9858 - accuracy: 0.6402 - val_loss: 1.1309 - val_accuracy: 0.6107 - lr: 3.1381e-04\n",
      "Epoch 93/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9846 - accuracy: 0.6424\n",
      "Epoch 93: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 235s 582ms/step - loss: 0.9846 - accuracy: 0.6424 - val_loss: 1.1390 - val_accuracy: 0.5998 - lr: 3.1381e-04\n",
      "Epoch 94/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9770 - accuracy: 0.6427\n",
      "Epoch 94: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 234s 580ms/step - loss: 0.9770 - accuracy: 0.6427 - val_loss: 1.1288 - val_accuracy: 0.6130 - lr: 2.8243e-04\n",
      "Epoch 95/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9673 - accuracy: 0.6477\n",
      "Epoch 95: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 234s 580ms/step - loss: 0.9673 - accuracy: 0.6477 - val_loss: 1.1385 - val_accuracy: 0.6014 - lr: 2.8243e-04\n",
      "Epoch 96/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9707 - accuracy: 0.6451\n",
      "Epoch 96: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 234s 580ms/step - loss: 0.9707 - accuracy: 0.6451 - val_loss: 1.1454 - val_accuracy: 0.6031 - lr: 2.8243e-04\n",
      "Epoch 97/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9680 - accuracy: 0.6494\n",
      "Epoch 97: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 235s 581ms/step - loss: 0.9680 - accuracy: 0.6494 - val_loss: 1.1257 - val_accuracy: 0.6173 - lr: 2.5419e-04\n",
      "Epoch 98/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9613 - accuracy: 0.6517\n",
      "Epoch 98: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 234s 580ms/step - loss: 0.9613 - accuracy: 0.6517 - val_loss: 1.1346 - val_accuracy: 0.6141 - lr: 2.5419e-04\n",
      "Epoch 99/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9452 - accuracy: 0.6549\n",
      "Epoch 99: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 234s 580ms/step - loss: 0.9452 - accuracy: 0.6549 - val_loss: 1.1402 - val_accuracy: 0.6090 - lr: 2.5419e-04\n",
      "Epoch 100/100\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9477 - accuracy: 0.6579\n",
      "Epoch 100: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 235s 581ms/step - loss: 0.9477 - accuracy: 0.6579 - val_loss: 1.1309 - val_accuracy: 0.6146 - lr: 2.2877e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "          train_data,\n",
    "          train_labels,\n",
    "          epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_split = 0.2,\n",
    "          shuffle = True,\n",
    "#          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
    "          callbacks=[lr_reducer, checkpointer]\n",
    "\n",
    "#          callbacks=[lr_reducer, early_stopper]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfWlxuYF6unl",
    "outputId": "38bb2792-38e3-4133-e7c5-349bfed4fed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.6140986347171914\n"
     ]
    }
   ],
   "source": [
    "predicted_test_labels = np.argmax(model.predict(test_data), axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "print (\"Accuracy score = \", accuracy_score(test_labels, predicted_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YnY9RqdyAN85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/content/My Drive/Colab Notebooks/Emotion Recognition/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9678 - accuracy: 0.6452\n",
      "Epoch 1: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 257s 636ms/step - loss: 0.9678 - accuracy: 0.6452 - val_loss: 1.1217 - val_accuracy: 0.6176 - lr: 2.2877e-04\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9604 - accuracy: 0.6468\n",
      "Epoch 2: val_loss did not improve from 1.11792\n",
      "404/404 [==============================] - 278s 688ms/step - loss: 0.9604 - accuracy: 0.6468 - val_loss: 1.1201 - val_accuracy: 0.6153 - lr: 2.2877e-04\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9517 - accuracy: 0.6522\n",
      "Epoch 3: val_loss improved from 1.11792 to 1.11555, saving model to /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\n",
      "INFO:tensorflow:Assets written to: /content/My Drive/Colab Notebooks/Emotion Recognition/Model\\weights.hd5\\assets\n",
      "404/404 [==============================] - 287s 711ms/step - loss: 0.9517 - accuracy: 0.6522 - val_loss: 1.1155 - val_accuracy: 0.6132 - lr: 2.2877e-04\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9432 - accuracy: 0.6558\n",
      "Epoch 4: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 324s 802ms/step - loss: 0.9432 - accuracy: 0.6558 - val_loss: 1.1214 - val_accuracy: 0.6169 - lr: 2.2877e-04\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9378 - accuracy: 0.6591\n",
      "Epoch 5: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 322s 797ms/step - loss: 0.9378 - accuracy: 0.6591 - val_loss: 1.1304 - val_accuracy: 0.6161 - lr: 2.2877e-04\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9327 - accuracy: 0.6611\n",
      "Epoch 6: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 322s 797ms/step - loss: 0.9327 - accuracy: 0.6611 - val_loss: 1.1189 - val_accuracy: 0.6121 - lr: 2.2877e-04\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.6598\n",
      "Epoch 7: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 321s 796ms/step - loss: 0.9274 - accuracy: 0.6598 - val_loss: 1.1293 - val_accuracy: 0.6146 - lr: 2.0589e-04\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9332 - accuracy: 0.6586\n",
      "Epoch 8: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 322s 798ms/step - loss: 0.9332 - accuracy: 0.6586 - val_loss: 1.1345 - val_accuracy: 0.6108 - lr: 2.0589e-04\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9196 - accuracy: 0.6642\n",
      "Epoch 9: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 321s 794ms/step - loss: 0.9196 - accuracy: 0.6642 - val_loss: 1.1381 - val_accuracy: 0.6088 - lr: 2.0589e-04\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - ETA: 0s - loss: 0.9215 - accuracy: 0.6661\n",
      "Epoch 10: val_loss did not improve from 1.11555\n",
      "404/404 [==============================] - 318s 788ms/step - loss: 0.9215 - accuracy: 0.6661 - val_loss: 1.1167 - val_accuracy: 0.6121 - lr: 1.8530e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "          train_data,\n",
    "          train_labels,\n",
    "          epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_split = 0.2,\n",
    "          shuffle = True,\n",
    "#          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
    "          callbacks=[lr_reducer, checkpointer]\n",
    "\n",
    "#          callbacks=[lr_reducer, early_stopper]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "t5edjLRP7AuL"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/model.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14328\\4201131819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Models/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mfcc_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Tools/haarcascade_frontalface_alt.xml\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14328\\4201131819.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mjson_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'model.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mloaded_model_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/model.json'"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_model(path):\n",
    "\n",
    "\tjson_file = open(path + 'model.json', 'r')\n",
    "\tloaded_model_json = json_file.read()\n",
    "\tjson_file.close()\n",
    "\t\n",
    "\tmodel = model_from_json(loaded_model_json)\n",
    "\tmodel.load_weights(path + \"model.h5\")\n",
    "\tprint(\"Loaded model from disk\")\n",
    "\treturn model\n",
    "\t\n",
    "def predict_emotion(gray, x, y, w, h):\n",
    "\tface = np.expand_dims(np.expand_dims(np.resize(gray[y:y+w, x:x+h]/255.0, (48, 48)),-1), 0)\n",
    "\tprediction = model.predict([face])\n",
    "\n",
    "\treturn(int(np.argmax(prediction)), round(max(prediction[0])*100, 2))\n",
    "\t\n",
    "path = \"Models/\"\n",
    "model = load_model(path)\n",
    "\n",
    "fcc_path = \"Tools/haarcascade_frontalface_alt.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(fcc_path)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\tret, frame = webcam.read()\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\n",
    "\tfaces = faceCascade.detectMultiScale(\n",
    "\t\t\tgray,\n",
    "\t\t\tscaleFactor=1.1,\n",
    "\t\t\tminNeighbors=5,\n",
    "\t\t\tminSize=(30, 30)\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\tfor (count,(x, y, w, h)) in enumerate(faces):\n",
    "\t\tcolour = colour_cycle[int(count%len(colour_cycle))]\n",
    "\t\tcv2.rectangle(frame, (x, y), (x+w, y+h), colour, 2)\n",
    "\t\tcv2.line(frame, (x+5, y+h+5),(x+100, y+h+5), colour, 20)\n",
    "\t\tcv2.putText(frame, \"Face #\"+str(count+1), (x+5, y+h+11), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), lineType=cv2.LINE_AA)\n",
    "\n",
    "\t\tcv2.line(frame, (x+8, y),(x+150, y), colour, 20)\n",
    "\t\temotion_id, confidence = predict_emotion(gray, x, y, w, h)\n",
    "\t\temotion = emotion_dict[emotion_id]\n",
    "\t\tcv2.putText(frame, emotion + \": \" + str(confidence) + \"%\" , (x+20, y+5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), lineType=cv2.LINE_AA)\n",
    "\t\n",
    "\tcv2.imshow('Emotion Recognition - Press q to exit.', frame)\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GrL-5BG-9AJ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jJ8nnoj-4tJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
